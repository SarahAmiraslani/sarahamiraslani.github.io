<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Predicting Top 10 Formula 1 Finishes and Clustering Race Tracks | Sarah H. Amiraslani </title> <meta name="author" content="Sarah H. Amiraslani"> <meta name="description" content="This project uses machine learning to predict top 10 finishes in F1 races and cluster tracks by characteristics."> <meta name="keywords" content="portfolio-website, sarah-amiraslani, data-science, analytics, data, quantitative, portfolio"> <meta property="og:site_name" content="Sarah H. Amiraslani"> <meta property="og:type" content="website"> <meta property="og:title" content="Sarah H. Amiraslani | Predicting Top 10 Formula 1 Finishes and Clustering Race Tracks"> <meta property="og:url" content="https://sarahamiraslani.github.io/projects/2_project/"> <meta property="og:description" content="This project uses machine learning to predict top 10 finishes in F1 races and cluster tracks by characteristics."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Predicting Top 10 Formula 1 Finishes and Clustering Race Tracks"> <meta name="twitter:description" content="This project uses machine learning to predict top 10 finishes in F1 races and cluster tracks by characteristics."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sarah H. Amiraslani"
        },
        "url": "https://sarahamiraslani.github.io/projects/2_project/",
        "@type": "WebSite",
        "description": "This project uses machine learning to predict top 10 finishes in F1 races and cluster tracks by characteristics.",
        "headline": "Predicting Top 10 Formula 1 Finishes and Clustering Race Tracks",
        
        "sameAs": ["https://github.com/SarahAmiraslani", "https://www.linkedin.com/in/samirasl", "https://medium.com/@Sarah Amiraslani", "https://gitlab.com/SarahAmiraslani", "https://www.kaggle.com/sarahamiraslani7"],
        
        "name": "Sarah H. Amiraslani",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?9a98343a170498a853a86a1345281c27"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%A9%F0%9F%8F%BB%E2%80%8D%F0%9F%92%BB&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sarahamiraslani.github.io/projects/2_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Predicting Top 10 Formula 1 Finishes and Clustering Race Tracks",
            "description": "This project uses machine learning to predict top 10 finishes in F1 races and cluster tracks by characteristics.",
            "published": "April 01, 2024",
            "authors": [
              
              {
                "author": "Sarah Amiraslani",
                "authorURL": "samirasl@umich.edu",
                "affiliations": [
                  {
                    "name": "University of Michigan, Ann Arbor",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Akshay Tharval",
                "authorURL": "tharval@umich.edu",
                "affiliations": [
                  {
                    "name": "University of Michigan, Ann Arbor",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Sam Kobrin",
                "authorURL": "kobrin@umich.edu",
                "affiliations": [
                  {
                    "name": "University of Michigan, Ann Arbor",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sarah</span> H. Amiraslani </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/resume/">Resume </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Predicting Top 10 Formula 1 Finishes and Clustering Race Tracks</h1> <p>This project uses machine learning to predict top 10 finishes in F1 races and cluster tracks by characteristics.</p> <p style="display: flex; align-items: center; height: 100%;"> <a href="https://github.com/SarahAmiraslani/formula1-predictions-track-clustering" target="_blank" rel="noopener noreferrer" aria-label="GitHub Repository"> <svg height="32" width="32" viewbox="0 0 16 16" version="1.1" aria-hidden="true"> <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.58.82-2.14-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.03 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.14 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"> </path> </svg> </a> </p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#data-source">Data Source</a> </div> <div> <a href="#preprocessing">Preprocessing</a> </div> <div> <a href="#exploratory-data-analysis">Exploratory Data Analysis</a> </div> <div> <a href="#feature-engineering">Feature Engineering</a> </div> <div> <a href="#modeling">Modeling</a> </div> <ul> <li> <a href="#top-10-predictions">Top 10 Predictions</a> </li> <li> <a href="#track-clustering">Track Clustering</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> </nav> </d-contents> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/red-bull.jpg" sizes="95vw"></source> <img src="/assets/img/red-bull.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/race-engineering.jpg" sizes="95vw"></source> <img src="/assets/img/race-engineering.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> (left) <a href="https://www.formula1.com/en/drivers/sergio-perez" rel="external nofollow noopener" target="_blank">Sergio Pérez</a> driving for Red Bull image captured by Jared C. Tilton<d-cite key="race_for_second_2024"></d-cite>. (right) <a href="https://en.wikipedia.org/wiki/Scuderia_AlphaTauri" rel="external nofollow noopener" target="_blank">Alpha Tauri</a> engineers analyzing lap times, GPS coordinates, and video footage in real time in the pit wall.<d-cite key="how_teams_analyse_f1_race_strategy_2024"></d-cite> </div> <h1 id="introduction">Introduction</h1> <p>Formula 1, often hailed as the pinnacle of motorsport, combines high-speed competition, cutting-edge technology, and a global fan base. This premier racing series showcases a unique synergy of engineering excellence, driver skill, and strategic depth, making it an intriguing subject for in-depth analysis. This project leverages machine learning to explore the complexities of Formula 1, with a primary focus on predicting top 10 finishes in races — a crucial outcome for securing championship points for drivers and teams. Additionally, we analyze race track characteristics through unsupervised learning, clustering tracks based on factors such as layout, surface, and length.</p> <p>Historically, Formula 1 analytics has focused on predicting race winners using various statistical and machine learning techniques. These studies often reflect the dominance of certain teams during specific eras, such as Red Bull’s supremacy from 2010 to 2013 and 2021 to the present, and Mercedes’ stronghold from 2013 to 2020. While insightful, this focus on predicting race winners can overlook the nuanced dynamics and strategic elements that contribute to the broader competitive landscape of Formula 1 racing.</p> <p>Our project aims to broaden the analytical scope by targeting the prediction of top 10 finishes. Securing a place in the top 10 is vital for accumulating championship points, crucial for both drivers and constructors over a season. By shifting our focus to these positions, we aim to capture the intricate competitive interactions and strategic nuances that influence race outcomes beyond the winner’s podium. This approach provides a more comprehensive understanding of performance determinants in Formula 1, addressing both the predictability associated with team dominance and the variability arising from tactical decisions across the grid.</p> <p>The Formula 1 points system awards points to the top 10 finishers in a grand prix, with the winner receiving 25 points, second place 18 points, and so on down to 1 point for the 10th place. Additionally, there is a bonus point for the fastest lap (FL) in the grand prix, provided the driver finishes in the top 10. In the shorter sprint races – about one-third the distance of a typical Grand Prix distance – points are awarded to the top 8 finishers, with the winner getting 8 points. This system, in place since 2010, was designed to create a larger gap between first and second place and to accommodate the expanded grid and improved reliability of modern cars.</p> <table> <thead> <tr> <th>Position</th> <th>1st</th> <th>2nd</th> <th>3rd</th> <th>4th</th> <th>5th</th> <th>6th</th> <th>7th</th> <th>8th</th> <th>9th</th> <th>10th</th> <th>FL</th> </tr> </thead> <tbody> <tr> <td>Points</td> <td>25</td> <td>18</td> <td>15</td> <td>12</td> <td>10</td> <td>8</td> <td>6</td> <td>4</td> <td>2</td> <td>1</td> <td>1</td> </tr> </tbody> </table> <p>Additionally, our exploration into the unsupervised clustering of race tracks adds another dimension to this analysis. By uncovering similarities and distinctions among tracks, we offer deeper insights into how different track characteristics influence racing strategies and outcomes. This dual-faceted approach distinguishes our work from conventional race prediction analyses, offering a novel perspective on the dynamics of Formula 1 racing.</p> <h1 id="data-source">Data Source</h1> <p>The Ergast Developer API serves as a pivotal data source for our Formula 1 analysis, offering an extensive repository of historical race data, including driver standings, race results, and qualifying times<d-cite key="ergast"></d-cite>. Renowned for its comprehensive coverage of F1 statistics, the API has been instrumental in various analytical projects, ranging from predictive modeling to detailed statistical analyses of driver performances. In our project, the Ergast API provided a robust foundation for both the supervised and unsupervised learning components. We used it to extract datasets spanning from 1995 to the present, reflecting our focus on the modern era of Formula 1 racing. This period is characterized by significant technological advancements and regulatory changes, making the data particularly relevant for our analysis. Key features of the API that we leveraged include its ability to filter data by race season, event, and individual driver/team performance metrics. This flexibility allowed us to tailor our dataset precisely to the needs of our predictive models and clustering algorithms, ensuring a high degree of accuracy and relevance in our analysis.</p> <blockquote> <h5 id="reproducibility-note">Reproducibility Note</h5> <p class="block-tip">The public Ergast API is set to sunset after the 2024 formula 1 season. To recreate You can find legacy data on <a href="https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020" rel="external nofollow noopener" target="_blank">Kaggle</a> or in this project’s <a href="https://github.com/SarahAmiraslani/formula1-predictions-track-clustering/tree/main/data/raw" rel="external nofollow noopener" target="_blank">GitHub repository</a>.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ergast_db.png" sizes="95vw"></source> <img src="/assets/img/ergast_db.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An entity-relationship (ER) model of all the data available for modeling using the Ergast API. </div> <h1 id="preprocessing">Preprocessing</h1> <h2 id="top-10-predictions">Top 10 Predictions</h2> <p>To prepare the dataset for supervised learning, a pre-processing routine was implemented, focusing on extracting and structuring key information from the Ergast Developer API. Initially, unique identifiers for races and racers were constructed using a combination of the race year and session for RaceId, and a concatenation of the racer’s first and last names for RacerId. These identifiers serve as indexes to uniquely identify data points without directly contributing to the model’s input features. For the Type of track, we categorized circuits into purpose-built tracks and street circuits, encoded as 0 and 1, respectively, based on circuit information obtained from the API. This binary classification aids the model in distinguishing the inherent differences between these track types. Additionally, we derived the TrackId by using the circuit name, ensuring a consistent reference across different datasets. The previous year’s race result for each racer, Qualifying position, and the timings for Q1, Q2, and Q3 were meticulously extracted and formatted, with non-participation or non-qualification explicitly marked, ensuring a comprehensive dataset ready for analysis.</p> <table class="styled-table"> <thead> <tr> <th>Column Name</th> <th>Description</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>RaceId</td> <td>Year-Session (e.g., 2021-Bahrain)</td> <td>Index</td> </tr> <tr> <td>RacerId</td> <td>First name-Last name (e.g., Lewis-Hamilton)</td> <td>Index</td> </tr> <tr> <td>TrackId</td> <td>Circuit name (e.g., Silverstone)</td> <td>Index</td> </tr> <tr> <td>Previous year result</td> <td>Racer's previous year result (0 if not available)</td> <td>Feature</td> </tr> <tr> <td>Qualifying position</td> <td>Racer's qualifying position</td> <td>Feature</td> </tr> <tr> <td>Q1 timing</td> <td>Qualifying round 1 timing</td> <td>Feature</td> </tr> <tr> <td>Q2 timing</td> <td>Qualifying round 2 timing (0 if did not qualify for Q2)</td> <td>Feature</td> </tr> <tr> <td>Q3 timing</td> <td>Qualifying round 3 timing (0 if did not qualify for Q3)</td> <td>Feature</td> </tr> <tr> <td>Race finish</td> <td>Race finish position</td> <td>Label</td> </tr> </tbody> </table> <h2 id="track-clustering">Track Clustering</h2> <p>Our clustering of circuits based on track characteristics provides insight into how tracks relate to one another across a number of characteristics. We employed unsupervised learning techniques to uncover latent groupings among F1 circuits worldwide.</p> <p>For our analysis of F1 track characteristics, we leveraged three data sources. The first was a table of F1 track characteristics scraped from wikipedia. The second was a track characteristic retrieved from the Ergast API. The third was a dataset of F1 circuits from Kaggle. Details of the data sources are provided in the table below.</p> <table class="styled-table"> <thead> <tr> <th style="width: 10%;">Attribute</th> <th style="width: 30%;">Wikipedia</th> <th style="width: 30%;">Ergast API</th> <th style="width: 30%;">Kaggle</th> </tr> </thead> <tbody> <tr> <td>Location</td> <td><a href="https://en.wikipedia.org/wiki/List_of_Formula_One_circuits" rel="external nofollow noopener" target="_blank">Wikipedia List of F1 Circuits</a></td> <td><a href="https://ergast.com/mrd/" rel="external nofollow noopener" target="_blank">API Query<d-footnote><code>https://ergast.com/api/f1/circuits?limit=100&amp;offset=0</code> was used as the API endpoint. However, note that the API service is deprecated as of 2024. </d-footnote></a></td> <td><a href="https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020" rel="external nofollow noopener" target="_blank">Kaggle F1 circuits.csv dataset</a></td> </tr> <tr> <td>Format</td> <td>scraped html table</td> <td>JSON</td> <td>CSV</td> </tr> <tr> <td>Important Variables Included</td> <td>Circuit Name, Type, Direction, circuit length in kilometers</td> <td>circuit name, altitude</td> <td>Number of turns</td> </tr> <tr> <td>Number of records</td> <td>77</td> <td>77</td> <td>80</td> </tr> <tr> <td>Time Period Covered</td> <td>1950-2023</td> <td>1950-2023</td> <td>1950-2023</td> </tr> <tr> <td>Preprocessing</td> <td>Extract track length from text string, convert to numeric</td> <td>Match track names from the Wikipedia table using Levenshtein distance, and perform a left join with the Wikipedia data.</td> <td>Match track names from the Wikipedia table using Levenshtein distance, and perform a left join with the Wikipedia data. . Impute missing number of turn values with K nearest neighbors imputer.</td> </tr> </tbody> </table> <p>After our initial data sourcing and preprocessing, we developed a dataset that contained 5 complete features for all 77 F1 circuits since 1950: altitude, track length in kilometers, number of turns, circuit type (road, race, or street), and circuit direction (clockwise, counterclockwise, or figure eight). Our selection of features to represent circuits was informed by consultations with F1 enthusiasts, ensuring our analysis represented how F1 subject matter experts would evaluate and characterize tracks.</p> <h1 id="exploratory-data-analysis">Exploratory Data Analysis</h1> <p>Our data analysis phase utilized a suite of visual tools to dissect and understand the underlying patterns within the Formula 1 dataset. Through the use of heatmaps, we were able to discern the correlation across various features, providing us with an initial glimpse into the relationships between variables such as qualifying times, previous year’s positions, and their impact on race outcomes. Pair plots further enriched our analysis by offering a granular view of the pairwise relationships between features, revealing trends, clusters, and potential outliers that could influence model performance. Additionally, leveraging feature importance plots, particularly from our Random Forest model, allowed us to identify the most predictive variables in determining top 10 finishes. This visual exploration not only guided our feature selection process but also offered profound insights into the factors that most significantly affect race performance, laying a robust foundation for our predictive modeling efforts. This comprehensive approach to data analysis ensures that our models are built on a nuanced understanding of the dataset, maximizing their ability to uncover meaningful patterns and predictions.</p> <h2 id="multivariate-analysis">Multivariate Analysis</h2> <p>A heatmap is a graphical representation of data where values are depicted by color, allowing for an intuitive perception of patterns, such as correlations between variables in a dataset, which can be crucial for identifying relationships and trends at a glance.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/f1-heatmap.png" sizes="95vw"></source> <img src="/assets/img/f1-heatmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <strong>Strong Negative Correlation between Qualifying Position and Race Position</strong>: There is a strong negative correlation (r = -0.51) between <code class="language-plaintext highlighter-rouge">qualifying_pos</code> and <code class="language-plaintext highlighter-rouge">position</code>. This suggests that drivers with better (lower number) qualifying positions tend to finish the race in better (also, numerically lower) positions.</li> <li> <strong>Positive Correlation between Q2 and Q3 Timings</strong>: The <code class="language-plaintext highlighter-rouge">q2_timing</code> and <code class="language-plaintext highlighter-rouge">q3_timing</code> have a strong positive correlation (r = 0.64). This indicates that drivers who perform well in Q2 also tend to perform well in Q3.</li> <li> <strong>Negative Correlation between Q2/Q3 Timings and Qualifying Position</strong>: Both <code class="language-plaintext highlighter-rouge">q2_timing</code> and <code class="language-plaintext highlighter-rouge">q3_timing</code> show a significant negative correlation with qualifying_pos (r = -0.48 and r = -0.64, respectively). This implies that faster (i.e., lower) timings in Q2 and Q3 are associated with better qualifying positions.</li> <li> <strong>Weak Correlation between Q1 Timing and Other Features</strong>: The <code class="language-plaintext highlighter-rouge">q1_timing</code> shows relatively weak correlations with other features, with the highest being r =0.17 with <code class="language-plaintext highlighter-rouge">q2_timing</code>. This might suggest that the Q1 timing is less indicative of the final position or the performances in the later qualifying rounds.</li> <li> <strong>Low Correlation between Previous Year Position and Current Position</strong>: There is a very weak negative correlation (almost zero) between <code class="language-plaintext highlighter-rouge">prev_year_pos</code> and <code class="language-plaintext highlighter-rouge">position</code>. This suggests that the previous year’s race position does not have a strong predictive power on the current year’s race outcome.</li> </ul> <p>A pair plot, or a scatterplot matrix, visualizes pairwise relationships between variables in a dataset, combining scatter plots for each variable combination with histograms to show the distribution of each variable, thus providing a comprehensive overview of correlations, trends, and distributions all in one figure.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/f1-pairplot.png" sizes="95vw"></source> <img src="/assets/img/f1-pairplot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <strong>Distinct Clusters in Timing Data</strong>: The scatter plots between <code class="language-plaintext highlighter-rouge">q2_timing</code> and <code class="language-plaintext highlighter-rouge">q3_timing</code> display distinct clusters, indicating that there may be distinct groups within the data. This could represent different performance tiers among the drivers or cars.</li> <li> <strong>Positive Relationship between Q2 and Q3</strong>: There is a positive linear relationship between <code class="language-plaintext highlighter-rouge">q2_timing</code> and <code class="language-plaintext highlighter-rouge">q3_timing</code>. Drivers who have lower (better) timings in Q2 also tend to have lower (better) timings in Q3.</li> <li> <strong>Top 10 Finishers Distribution</strong>: When looking at the hue for <code class="language-plaintext highlighter-rouge">position</code>, there appears to be a concentration of top 10 finishers (denoted by the color, possibly orange) with lower <code class="language-plaintext highlighter-rouge">qualifying_pos</code> values, reinforcing the importance of qualifying performance on race outcomes.</li> <li> <strong>Timing and Qualifying Position</strong>: There is a trend visible in the scatter plots of the timing variables (q1_timing, <code class="language-plaintext highlighter-rouge">q2_timing</code>, <code class="language-plaintext highlighter-rouge">q3_timing</code>) against qualifying_pos. Drivers with lower qualifying positions tend to have lower (faster) timings in all three qualifying sessions, particularly noticeable in <code class="language-plaintext highlighter-rouge">q2_timing</code> and <code class="language-plaintext highlighter-rouge">q3_timing</code>.</li> </ul> <h2 id="feature-importance">Feature Importance</h2> <p>A feature importance plot ranks the features of a model based on their importance in making accurate predictions, and in the context of a Random Forest Classifier, it reflects how much each feature contributes to reducing the variance in the model’s predictions.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/f1-feature-importance.png" sizes="95vw"></source> <img src="/assets/img/f1-feature-importance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ul> <li> <strong>Q1 Timing Dominance</strong>: The <code class="language-plaintext highlighter-rouge">q1_timing</code> feature has the highest importance score, suggesting it is the most significant predictor in determining the race position.</li> <li> <strong>Lower Importance of Previous Year Position</strong>: The <code class="language-plaintext highlighter-rouge">prev_year_pos</code> feature has the lowest importance score, indicating that it has the least influence on the model’s predictions of race position outcomes compared to the other features in the dataset.</li> </ul> <h1 id="feature-engineering">Feature Engineering</h1> <h2 id="top-10-predictions-1">Top 10 predictions</h2> <h2 id="track-clustering-1">Track Clustering</h2> <p>Because clustering and dimensionality reduction algorithms require numeric representations of data, our categorical features were one-hot encoded. This transformation allowed us to integrate categorical and continuous data, enhancing the robustness of our analysis. Furthermore, to ensure features with larger scales did not dominate the cluster distance calculations, potentially biasing the clustering outcome, we leveraged a standard scaler to ensure that each feature contributes equally to the distance computations.</p> <h1 id="modeling">Modeling</h1> <h2 id="top-10-predictions-2">Top 10 Predictions</h2> <p>Supervised learning is at the core of our project, focusing on predicting the likelihood of Formula 1 drivers finishing in the top 1 — an outcome critical for championship points. Our methodology involves training models on a dataset featuring variables such as track type, drivers’ past performances, qualifying positions, and lap times. The goal is to classify each race outcome into a binary variable indicating whether a driver finishes in the top 10. This focus is driven by the significant impact these positions have on championship standings.</p> <p>We employ various models, including Random Forest, Logistic Regression, and Neural Networks, each chosen for its ability to capture different aspects of the complex dynamics influencing Formula 1 race outcomes. These models are trained, validated, and tested on data from the Ergast Developer API, providing a robust foundation for our predictive analysis. Accuracy is our primary metric for evaluating model performance, as it offers clear interpretability for our binary classification task, simplifying the analysis by focusing on the overall goal of identifying drivers and conditions that frequently lead to top 10 finishes.</p> <h3 id="logistic-regression">Logistic Regression</h3> <p>Logistic Regression is a statistical method used to predict a binary outcome based on one or more predictor variables. It is particularly popular for binary classification tasks, such as predicting whether a driver will finish in the top 10.</p> <details><summary>Click here to see the code block</summary> <p>The function below was used to find the ideal the logistic regression model using grid search.</p> <d-code block="" language="python"> def build_best_logistic_regression_model( X_train_scaled, X_test_scaled, y_train, y_test, param_grid, \*\*grid_search_kwargs ): """ Builds and returns the best Logistic Regression model using GridSearchCV. Args: X_train_scaled (pd.DataFrame): Scaled training features. X_test_scaled (pd.DataFrame): Scaled test features. y_train (pd.Series): Training labels. y_test (pd.Series): Test labels. param_grid (dict): Parameter grid for GridSearchCV. **grid_search_kwargs: Additional keyword arguments for GridSearchCV. Returns: best_model (LogisticRegression): The best Logistic Regression model. accuracy (float): Accuracy of the best model on the test set. """ # Initializing the Logistic Regression model log_reg = LogisticRegression(random_state=42, max_iter=1000) # Setting up GridSearchCV to find the best model grid_search = GridSearchCV( estimator=log_reg, param_grid=param_grid, cv=5, scoring="accuracy", n_jobs=-1, verbose=1, **grid_search_kwargs, ) start_time = time.time() logging.info("Starting GridSearchCV to find the best Logistic Regression model.") try: # Fitting GridSearchCV to the training data grid_search.fit(X_train_scaled, y_train) # Extracting the best estimator (model) best_model = grid_search.best_estimator_ # Making predictions with the best model on the test set y_pred = best_model.predict(X_test_scaled) # Calculating the accuracy of the best model accuracy = accuracy_score(y_test, y_pred) end_time = time.time() elapsed_time = end_time - start_time logging.info(f"GridSearchCV completed in {elapsed_time:.2f} seconds.") logging.info(f"Best Model's Accuracy: {accuracy * 100:.2f}%") logging.info(f"Best Parameters: {grid_search.best_params_}") # Returning the best model and its accuracy return best_model, accuracy except Exception as e: logging.error(f"An error occurred during GridSearchCV: {e}") return None, None # Defining the parameter grid for logistic regression lr_param_grid = { "C": [0.01, 0.1, 1, 10, 100], "penalty": ["l1", "l2"], "solver": ["liblinear"], # 'liblinear' is compatible with l1 and l2 penalties. } # Building and evaluating the Logistic Regression model best_lr_model, best_lr_accuracy = build_best_logistic_regression_model( X_train_scaled, X_test_scaled, y_train, y_test, lr_param_grid, return_train_score=True, ) if best_lr_model is not None: dump(best_lr_model, "best_logistic_regression_model.joblib") </d-code> </details> <p>The provided code defines a function to construct and fine-tune a logistic regression model using grid search to explore a defined space of hyperparameters. GridSearchCV fits the model on the scaled training data using combinations of regularization strength (<code class="language-plaintext highlighter-rouge">C</code>), penalty type (<code class="language-plaintext highlighter-rouge">penalty</code>), and solver algorithm (<code class="language-plaintext highlighter-rouge">solver</code>) to determine the most effective parameters. The optimal model, with an l1 penalty, a regularization strength of 0.01, and the liblinear solver, achieved a prediction accuracy of 73.14% on the test dataset.</p> <h3 id="random-forest">Random Forest</h3> <p>Random Forest is an ensemble learning method that constructs multiple decision trees and outputs the mode of the classes for classification tasks. It is known for its accuracy, robustness, and ability to handle large datasets with high dimensionality, reducing the risk of overfitting.</p> <details><summary>Click here to see the code block</summary> <d-code block="" language="python"> def build_best_random_forest_model( X_train, X_test, y_train, y_test, param_grid, \*\*grid_search_kwargs ): """ Builds and returns the best Random Forest model using GridSearchCV. Args: X_train (pd.DataFrame): Training features. X_test (pd.DataFrame): Test features. y_train (pd.Series): Training labels. y_test (pd.Series): Test labels. param_grid (dict): Parameter grid for GridSearchCV. **grid_search_kwargs: Additional keyword arguments for GridSearchCV. Returns: best_model (RandomForestClassifier): The best Random Forest model. accuracy (float): Accuracy of the best model on the test set. """ # Initializing the Random Forest classifier rf_clf = RandomForestClassifier(random_state=42) # Setting up GridSearchCV to find the best model grid_search = GridSearchCV( estimator=rf_clf, param_grid=param_grid, cv=5, scoring="accuracy", n_jobs=-1, verbose=1, **grid_search_kwargs, ) start_time = time.time() logging.info("Starting GridSearchCV to find the best Random Forest model.") try: # Fitting GridSearchCV to the training data grid_search.fit(X_train, y_train) # Extracting the best estimator (model) best_model = grid_search.best_estimator_ # Making predictions with the best model on the test set y_pred = best_model.predict(X_test) # Calculating the accuracy of the best model accuracy = accuracy_score(y_test, y_pred) end_time = time.time() elapsed_time = end_time - start_time logging.info(f"GridSearchCV completed in {elapsed_time:.2f} seconds.") logging.info(f"Best Model's Accuracy: {accuracy * 100:.2f}%") logging.info(f"Best Parameters: {grid_search.best_params_}") # Returning the best model and its accuracy return best_model, accuracy except Exception as e: logging.error(f"An error occurred during GridSearchCV: {e}") return None, None # Defining the parameter grid for Random Forest rf_param_grid = { "n_estimators": [10, 50, 100, 200], "max_depth": [None, 10, 20, 30], "min_samples_split": [2, 5, 10], } # Example usage with additional GridSearchCV parameters best_rf_model, best_rf_accuracy = build_best_random_forest_model( X_train_scaled, X_test_scaled, y_train, y_test, rf_param_grid, return_train_score=True, ) if best_rf_model is not None: dump(best_rf_model, "best_random_forest_model.joblib") </d-code> </details> <p>The provided code builds and evaluates a Random Forest classifier using a grid search over specified hyperparameters. GridSearchCV systematically tests parameter combinations, cross-validating to find the best performance in terms of accuracy. The optimal model achieved an accuracy of 73.87% on the test set, with 200 trees (<code class="language-plaintext highlighter-rouge">n_estimators</code>), a maximum depth of 10 (<code class="language-plaintext highlighter-rouge">max_depth</code>), and a minimum split size of 10 (<code class="language-plaintext highlighter-rouge">min_samples_split</code>).</p> <h3 id="neural-networks">Neural Networks</h3> <p>Neural Networks are algorithms modeled after the human brain, designed to recognize patterns and perform classification tasks. They process information in a layered structure of nodes, making them effective for complex pattern recognition and predictive modeling.</p> <details><summary>Click here to see the code block</summary> <d-code block="" language="python"> def create_model(hidden_nodes=1, learning_rate=0.001): """ Creates and compiles a Keras Sequential model. Args: hidden_nodes (int): Number of nodes in the hidden layer. learning_rate (float): Learning rate for the optimizer. Returns: model (Sequential): Compiled Keras model. """ # Define the model model = Sequential() model.add(Dense(hidden_nodes, input_dim=X_train_scaled.shape[1], activation="relu")) model.add(Dense(1, activation="sigmoid")) # Output layer for binary classification # Compile the model optimizer = Adam(learning_rate=learning_rate) model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"]) return model def build_best_neural_network_model( X_train_scaled, X_test_scaled, y_train, y_test, param_grid, \*\*grid_search_kwargs ): """ Builds and returns the best neural network model using GridSearchCV. Args: X_train_scaled (pd.DataFrame): Scaled training features. X_test_scaled (pd.DataFrame): Scaled test features. y_train (pd.Series): Training labels. y_test (pd.Series): Test labels. param_grid (dict): Parameter grid for GridSearchCV. **grid_search_kwargs: Additional keyword arguments for GridSearchCV. Returns: best_model (KerasClassifier): The best neural network model. accuracy (float): Accuracy of the best model on the test set. """ # Wrapping the Keras model so it can be used by scikit-learn model = KerasClassifier(build_fn=create_model, verbose=0) # Setting up GridSearchCV to find the best model grid_search = GridSearchCV( estimator=model, param_grid=param_grid, cv=5, scoring="accuracy", n_jobs=-1, verbose=1, **grid_search_kwargs, ) start_time = time.time() logging.info("Starting GridSearchCV to find the best neural network model.") try: # Fitting GridSearchCV to the training data grid_search.fit(X_train_scaled, y_train) # Extracting the best estimator (model) best_model = grid_search.best_estimator_ # Making predictions with the best model on the test set y_pred = best_model.predict(X_test_scaled) y_pred = np.round(y_pred).astype(int) # Convert probabilities to binary output # Calculating the accuracy of the best model accuracy = accuracy_score(y_test, y_pred) end_time = time.time() elapsed_time = end_time - start_time logging.info(f"GridSearchCV completed in {elapsed_time:.2f} seconds.") logging.info(f"Best Model's Accuracy: {accuracy * 100:.2f}%") logging.info(f"Best Parameters: {grid_search.best_params_}") # Returning the best model and its accuracy return best_model, accuracy except Exception as e: logging.error(f"An error occurred during GridSearchCV: {e}") return None, None # Defining the parameter grid for the neural network nn_param_grid = { "hidden_nodes": [10, 50, 100], "learning_rate": [0.001, 0.01, 0.1], "epochs": [50], "batch_size": [32], } # Building and evaluating the neural network model best_nn_model, best_nn_accuracy = build_best_neural_network_model( X_train_scaled, X_test_scaled, y_train, y_test, nn_param_grid, return_train_score=True, ) if best_nn_model is not None: dump(best_nn_model, "best_neural_network_model.joblib") </d-code> </details> <p>The provided code builds and optimizes a neural network model for binary classification using Keras and TensorFlow. A function, <code class="language-plaintext highlighter-rouge">create_model</code>, defines a neural network with one hidden layer and a given learning rate. The <code class="language-plaintext highlighter-rouge">build_best_neural_network_model</code> function uses <code class="language-plaintext highlighter-rouge">GridSearchCV</code> to iterate over hyperparameters such as the number of hidden nodes, learning rate, epochs, and batch size. The best-performing model, with 10 hidden nodes, a learning rate of 0.01, a batch size of 32, and training for 50 epochs, achieved an accuracy of 73.82% on the scaled test data.</p> <h3 id="sensitivity-analysis">Sensitivity Analysis</h3> <p>The sensitivity analysis across all three models — Random Forest, Logistic Regression, and Neural Network — reveals consistent findings. The feature <code class="language-plaintext highlighter-rouge">qualifying_pos</code> exhibits the highest sensitivity, indicating a strong influence on the model’s predictions; changes in qualifying position are likely to have a significant impact on the probability of a driver finishing in the top 10. In contrast, <code class="language-plaintext highlighter-rouge">prev_year_pos</code> shows notably lower sensitivity, suggesting that a driver’s position in the previous year is less indicative of their performance in the current race. The timing features (<code class="language-plaintext highlighter-rouge">q1_timing</code>, <code class="language-plaintext highlighter-rouge">q2timing</code>, <code class="language-plaintext highlighter-rouge">q3_timibg</code>) demonstrate moderate sensitivity, with some variability across the models, reflecting their respective influence on race outcomes. This consistent pattern across different models underscores the robustness of these features’ influences on predicting top 10 finishes in Formula 1 races.</p> <details><summary>Click here to see the code block</summary> <d-code block="" language="python"> def perform_sensitivity_analysis( models, X_scaled, feature_names, output_dir=".", file_format="png" ): """ Perform sensitivity analysis on given models. Args: models (dict): Dictionary of trained models with their names as keys. X_scaled (np.ndarray): Scaled features array. feature_names (list): List of feature names. output_dir (str): Directory to save the sensitivity analysis plots. file_format (str): File format for saving plots. """ # Number of points to evaluate for each feature num_points = 100 # Results dictionary to store sensitivity data sensitivity_results = {} # Iterate through each model for model_name, model in models.items(): logging.info(f"Analyzing model: {model_name}") sensitivity_results[model_name] = {} # Iterate through each feature for feature in feature_names: # Array to hold predictions predictions = [] # Generate values across the range feature_index = feature_names.index(feature) min_val, max_val = np.min(X_scaled[:, feature_index]), np.max( X_scaled[:, feature_index] ) values = np.linspace(min_val, max_val, num_points) # Modify one feature at a time, keeping others constant for val in values: X_temp = np.copy(X_scaled) X_temp[:, feature_index] = val try: pred = model.predict(X_temp) predictions.append(np.mean(pred)) except Exception as e: logging.error( f"Error predicting with model {model_name} for feature {feature}: {e}" ) predictions.append(np.nan) # Use NaN to indicate prediction failure sensitivity_results[model_name][feature] = np.nanstd( predictions ) # Use nanstd to handle NaNs # Plotting the sensitivity results for model_name, sensitivities in sensitivity_results.items(): plt.figure(figsize=(10, 6)) plt.title(f"Sensitivity Analysis for {model_name}") plt.bar(range(len(sensitivities)), list(sensitivities.values()), align="center") plt.xticks( range(len(sensitivities)), list(sensitivities.keys()), rotation="vertical" ) plt.ylabel("Standard Deviation of Predictions") file_path = f"{output_dir}/sensitivity_{model_name}.{file_format}" plt.savefig(file_path, dpi=300) plt.show() logging.info(f"Sensitivity plot saved to {file_path}") # Example usage models = { "Random Forest": best_rf_model, "Logistic Regression": best_lr_model, "Neural Network": best_nn_model, } feature_names = [ "q1_timing", "q2_timing", "q3_timing", "qualifying_pos", "prev_year_pos", ] perform_sensitivity_analysis(models, X_test_scaled, feature_names) </d-code> </details> <h3 id="failure-analysis">Failure Analysis</h3> <p>Supervised learning, while a powerful tool for predictive modeling, may encounter significant challenges in the context of Formula 1 race outcome predictions, primarily due to the intricacies and complexities inherent to the sport. One of the fundamental hurdles is the inadequacy of available data concerning the detailed specifications and configurations of F1 cars. This information, encompassing aspects like downforce levels, ground clearance, engine settings, and more, remains proprietary to each racing team. Such data is critical for accurately forecasting race outcomes, as these variables can significantly impact a car’s performance on different tracks under varying conditions. Moreover, the dynamic nature of F1 races, where strategies and car setups are meticulously tailored for each race weekend, further complicates the predictive modeling efforts. Without access to this depth of data, supervised learning models are limited to more surface-level features, potentially overlooking nuanced factors that are decisive for race results.</p> <p>Additionally, our approach deliberately excludes pit stop strategies from the predictive factors, considering them as in-race variables that are not predetermined and thus fall outside the scope of pre-race predictions. This decision, while simplifying the modeling process, omits a crucial element of race strategy that can dramatically influence race outcomes. Pit stops are often used strategically to gain an advantage over competitors, and their timing and execution can vary based on a multitude of factors, including weather conditions and tire performance. Furthermore, the unpredictable nature of mechanical failures and retirements adds another layer of complexity to race outcome predictions. Cars may fail for myriad reasons, from engine blowouts to collisions, which are virtually impossible to predict with supervised learning models based solely on historical data. These elements introduce a level of randomness and uncertainty that can derail even the most sophisticated models, underscoring the limitations of using supervised learning for predicting outcomes in a sport as complex and multifaceted as Formula 1.</p> <h2 id="track-clustering-2">Track Clustering</h2> <p>Our primary clustering algorithm was DBSCAN (Density-Based Spatial Clustering of Applications with Noise). This unsupervised clustering algorithm is known for its efficacy in identifying non-normally distributed clusters of varying shapes and sizes without predefining the number of clusters. The optimal epsilon value, a critical hyperparameter for DBSCAN, was estimated using the Nearest Neighbors algorithm. We adhered to the heuristic of setting <code class="language-plaintext highlighter-rouge">n_neighbors</code> to <code class="language-plaintext highlighter-rouge">2*dim - 1</code>, where <code class="language-plaintext highlighter-rouge">dim</code> represents the dimensionality of our feature space, to balance local density estimation and computational feasibility. We determined the optimal epsilon value by evaluating where the distance of the 5th nearest neighbor stated to increase exponentially in the K-distance graph.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Dendogram-track.png" sizes="95vw"></source> <img src="/assets/img/Dendogram-track.png" class="img-fluid rounded z-depth-1 same-height-sd" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/K-Distance.png" sizes="95vw"></source> <img src="/assets/img/K-Distance.png" class="img-fluid rounded z-depth-1 same-height-sd" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Evaluation of Clusters with Elbow method and Dendrogram. </div> <p>Simultaneously, we explored hierarchical clustering to validate our findings with DBSCAN. Through visual inspection of a dendrogram, we optimized the distance threshold to ensure that there was significant dissimilarity between clusters. Despite some variance, both DBSCAN and hierarchical clustering revealed similar cluster structures, underscoring the generalizability of the clusters produced by DBSCAN.</p> <p>To evaluate the sensitivity of our clusters to changes in the data, we took three bootstrap random samples of 80% of the data. These bootstrap samples yielded similar clusters to clusters generated from the entire dataset, denoting the stability of our clustering approach to changes in the data.</p> <p>To navigate the high-dimensional nature of our data, we leveraged Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding t-SNE for dimensionality reduction to visualize in two-dimensions. While PCA provided a broad overview of data variance, t-SNE allowed us to observe the nuanced structure of our clusters in two dimensions. Our biplot overlay on the PCA scatter plot revealed that circuit types and directions were predominant drivers behind the clustering.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tracks-biplot.png" sizes="95vw"></source> <img src="/assets/img/tracks-biplot.png" class="img-fluid rounded z-depth-1 same-height-sd" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/t-sne-tracks.png" sizes="95vw"></source> <img src="/assets/img/t-sne-tracks.png" class="img-fluid rounded z-depth-1 same-height-sd" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Visual evaluation of cluster results: track direction and type drives clusters. </div> <h1 id="discussion">Discussion</h1> <h2 id="top-10-prediction">Top 10 Prediction</h2> <p>The Random Forest model, with its ensemble approach, provided an accuracy of 73.87%, indicating a robust predictive capability potentially due to its handling of non-linear relationships between features and the target variable. This model’s performance was slightly outperformed by the Neural Network model, which achieved an accuracy of 73.82%, with an architecture of a single hidden layer containing ten nodes. The Neural Network’s performance, despite being marginally lower, suggests that a more complex model does not necessarily guarantee superior results, considering the computational complexity and time taken to train such models. The Logistic Regression model, a more straightforward and interpretable model, yielded a slightly lower accuracy of 73.14%. This could be attributed to its linear nature, which may not capture complex patterns in the data as effectively as the other two models. However, the difference in accuracy is relatively small, highlighting that simpler models can still be competitive and beneficial, especially when considering the trade-off between performance and model complexity.</p> <p>The feature importance plot and the pair plot provided valuable insights into the predictive power of different features, with q1_timing and qualifying_pos being the most significant predictors across models, aligning with domain knowledge that qualifying performances have substantial impacts on race outcomes. The negligible importance of the prev_year_pos feature suggests that past performance is not a reliable indicator of future race results, pointing to the dynamic nature of the sport where numerous variables can influence race day performance.</p> <h2 id="track-clustering-3">Track Clustering</h2> <p>In our unsupervised analysis of clustering Formula 1 circuits, tuning hyperparameters in the DBSCAN algorithm emerged as a significant factor in achieving meaningful clusters. This aspect underscores that adjustment of algorithm parameters can profoundly impact the outcomes and interpretability of the results. However, we encountered a remarkable challenge when categorical variables, rather than continuous variables like track length, number of turns, and altitude, predominantly drove the variation in clusters. This observation was somewhat anticipated, given the relative uniformity in these continuous variables across different tracks, which consequently led to categorical factors becoming more influential in the clustering process. This outcome, while initially disappointing, provided valuable insights into the feature space of track characteristics. In light of these findings, future work should pivot towards exploring the clustering of driver finish times in relation to track characteristics. This direction aims to delve deeper into the potential correlations between track features and performance outcomes, potentially uncovering patterns that could inform strategic decisions in race preparation and execution.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/formula-one-distill.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"SarahAmiraslani/sarahamiraslani.github.io","data-repo-id":"R_kgDOL8JPNA","data-category":"General","data-category-id":"DIC_kwDOL8JPNM4CfaoA","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sarah H. Amiraslani. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: February 18, 2025. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PPKD31SS2Z"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-PPKD31SS2Z");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?ed714bdd9e08375c6c326bb17f4f7f47"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of my favorite projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-resume",title:"Resume",description:"Below is a summary of my professional contributions, skills, and accomplishments. For portability and more detailed information, please click the pdf icon in the upper right to download my latest resume.",section:"Navigation",handler:()=>{window.location.href="/resume/"}},{id:"nav-repositories",title:"Repositories",description:"Selected code samples and GitHub statistics. For more detailed information, visit my Github profile.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-exploring-breaking-bad-relations",title:"Exploring Breaking Bad Relations",description:"Visualizing character relations",section:"Posts",handler:()=>{window.location.href="/blog/2024/breaking-bad-relations/"}},{id:"post-unraveling-complexity-network-science-applied-to-star-wars-films",title:"Unraveling Complexity: Network Science Applied to Star Wars Films",description:"This blog post explores the fundamentals of Network Analysis using the Star Wars character interactions.",section:"Posts",handler:()=>{window.location.href="/blog/2024/starwars-network/"}},{id:"news-starting-my-new-role-at-nvidia-working-on-data-engineering-and-analytics-pipelines",title:"Starting my new role at NVIDIA working on data engineering and analytics pipelines!",description:"",section:"News"},{id:"projects-draw-to-learn-a-strategy-to-learn-abstract-concepts",title:"Draw to Learn: A Strategy to Learn Abstract Concepts",description:"This project studies how student-generated drawings aid learning of abstract science concepts in a lab setting.",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-predicting-top-10-formula-1-finishes-and-clustering-race-tracks",title:"Predicting Top 10 Formula 1 Finishes and Clustering Race Tracks",description:"This project uses machine learning to predict top 10 finishes in F1 races and cluster tracks by characteristics.",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-predicting-sunspots",title:"Predicting Sunspots",description:"This project leverages ensemble and deep learning models to forecast sunspots and Heliospheric Current Sheet (HCS) indexes, enhancing the prediction of solar wind structures.",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-tracing-the-origins-of-solar-wind",title:"Tracing the Origins of Solar Wind",description:"This project uses machine learning to analyze solar wind, trace its solar corona origins, and predict heliophysical events.",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%61%72%61%68%61%6D%69%72%61%73%6C%61%6E%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/SarahAmiraslani","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/samirasl","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@Sarah Amiraslani","_blank")}},{id:"socials-gitlab",title:"GitLab",section:"Socials",handler:()=>{window.open("https://gitlab.com/SarahAmiraslani","_blank")}},{id:"socials-kaggle",title:"Kaggle",section:"Socials",handler:()=>{window.open("https://www.kaggle.com/sarahamiraslani7","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> </body> </html>