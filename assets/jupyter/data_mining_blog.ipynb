{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages and dependencies that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import re\n",
    "from typing import Dict\n",
    "import warnings\n",
    "\n",
    "# Related third party imports\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import plotly.express as px\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import squarify\n",
    "\n",
    "# set the default style for the plots and supress warnings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains a sample of roughly 10 thousand Tweets that contain two or more food or drink emojis. We will represent the emojis in these tweets as a collection of itemsets and calulate their similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the local file that contains sampled tweets\n",
    "tweets_df = pd.read_csv(\n",
    "    \"assets/food_drink_emoji_tweets.txt\", sep=\"\\t\", header=None\n",
    ").rename(columns={0: \"text\"})\n",
    "\n",
    "print(tweets_df.shape)\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every line of this data is a tweet. There are a few more features that we can extract from this dataset that may give us context into the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_unique_emojis = set()\n",
    "for text in tweets_df['text']:\n",
    "    emojis = emoji.emoji_list(text)\n",
    "    for emoji_data in emojis:\n",
    "        tweets_unique_emojis.add(emoji_data['emoji'])\n",
    "\n",
    "print(tweets_unique_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count emojis per tweet (including diuplicates)\n",
    "tweets_df[\"emoji_count\"] = tweets_df[\"text\"].apply(emoji.emoji_count)\n",
    "\n",
    "# Calculate the central tendancy stats\n",
    "average_emojis = tweets_df[\"emoji_count\"].mean()\n",
    "median_emoji = tweets_df[\"emoji_count\"].median()\n",
    "\n",
    "sns.histplot(tweets_df[\"emoji_count\"], bins=range(0, 60))\n",
    "plt.xlabel(\"Number of emojis in tweet\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.title(\"Distribution of emoji count in tweets\")\n",
    "\n",
    "# Add a vertical line at the average number of emojis\n",
    "plt.axvline(average_emojis, color=\"r\", linestyle=\"dashed\", linewidth=1)\n",
    "\n",
    "# Add a text annotation\n",
    "plt.annotate(\n",
    "    \"Average: {:.2f}\".format(average_emojis),\n",
    "    xy=(average_emojis, 0),\n",
    "    xytext=(average_emojis + 1, 2550),\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it would be ideal to organize the emojis witnessed into semantic categories. One consistent way of doing so is by pulling the unicode categories from unicode.org. The code below does so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_emoji_data():\n",
    "    # Use a context manager for the request\n",
    "    with requests.get(\n",
    "        \"https://unicode.org/Public/emoji/13.1/emoji-test.txt\"\n",
    "    ) as response:\n",
    "        lines = response.text.split(\"\\n\")\n",
    "\n",
    "    current_group = current_subgroup = None\n",
    "    emoji_categories = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"# group:\"):\n",
    "            current_group = line.split(\": \")[1]\n",
    "        elif line.startswith(\"# subgroup:\"):\n",
    "            current_subgroup = line.split(\": \")[1]\n",
    "        elif line and not line.startswith(\"#\"):\n",
    "            emoji_str = re.findall(r\"#\\s(.+)\", line.split(\";\")[1])[0].split()[0].strip()\n",
    "            emoji_categories[emoji_str] = {\n",
    "                \"group\": current_group,\n",
    "                \"subgroup\": current_subgroup,\n",
    "            }\n",
    "\n",
    "    return emoji_categories\n",
    "\n",
    "emoji_categories = fetch_emoji_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all possible emojis in a set that will remain constant\n",
    "EMOJI_SET = set(emoji_categories.keys())\n",
    "\n",
    "# explore categories\n",
    "print(emoji_categories[emoji.emojize(\":grinning_face_with_big_eyes:\")])\n",
    "print(emoji_categories[\"ðŸ˜ƒ\"]) # equivalent to line above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all emojis, categories and subgroups in a dataframe for convenience\n",
    "emoji_df = pd.DataFrame.from_dict(emoji_categories, orient=\"index\").reset_index()\n",
    "emoji_df.columns = [\"emoji\", \"category\", \"subgroup\"]\n",
    "emoji_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_emoji_categories_df = emoji_df[emoji_df[\"emoji\"].isin(tweets_unique_emojis)]\n",
    "tweets_emoji_categories_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_uniq_emojis(text: str, emoji_set: set) -> np.array:\n",
    "    \"\"\"Extracts unique emojis from the text.\"\"\"\n",
    "    return np.unique([chr for chr in text if chr in emoji_set])\n",
    "\n",
    "\n",
    "def extract_uniq_emoji_categories(\n",
    "    text: str, emoji_set: set, emoji_categories: Dict[str, Dict[str, str]]\n",
    ") -> np.array:\n",
    "    \"\"\"Extracts unique emoji categories from the text.\"\"\"\n",
    "    return np.unique(\n",
    "        [emoji_categories[chr][\"group\"] for chr in text if chr in emoji_set]\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_uniq_emojis_subgroups(\n",
    "    text: str, emoji_set: set, emoji_categories: Dict[str, Dict[str, str]]\n",
    ") -> np.array:\n",
    "    \"\"\"Extracts unique emoji subgroups from the text.\"\"\"\n",
    "    return np.unique(\n",
    "        [emoji_categories[chr][\"subgroup\"] for chr in text if chr in emoji_set]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"emojis\"] = tweets_df[\"text\"].apply(lambda x: extract_uniq_emojis(x, EMOJI_SET))\n",
    "tweets_df[\"emoji_categories\"] = tweets_df.text.apply(\n",
    "    lambda x: extract_uniq_emoji_categories(x, EMOJI_SET, emoji_categories)\n",
    ")\n",
    "tweets_df[\"emoji_subgroups\"] = tweets_df.text.apply(\n",
    "    lambda x: extract_uniq_emojis_subgroups(x, EMOJI_SET, emoji_categories)\n",
    ")\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of categories and calculate the counts\n",
    "category_counts = tweets_df[\"emoji_categories\"].explode().value_counts()\n",
    "subcategories_counts = tweets_df[\"emoji_subgroups\"].explode().value_counts()\n",
    "\n",
    "# Create a treemap\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "squarify.plot(sizes=category_counts.values, label=category_counts.index, alpha=0.6)\n",
    "plt.title(\"Treemap of Emoji Categories\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.sunburst(\n",
    "    tweets_emoji_categories_df, path=[\"category\", \"subgroup\"], color=\"subgroup\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "emoji_matrix = pd.DataFrame(\n",
    "    data=mlb.fit_transform(tweets_df.emojis),\n",
    "    index=tweets_df.index,\n",
    "    columns=mlb.classes_,\n",
    ")\n",
    "emoji_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_length = emoji_matrix.sum(axis=1)\n",
    "tweets_df.join(emoji_length.rename(\"emoji_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.join(emoji_length.rename(\"emoji_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_length_distribution = (\n",
    "    emoji_length.value_counts()\n",
    "    .rename_axis(index=\"emoji_length\")\n",
    "    .rename(\"freq\")\n",
    "    .reset_index()\n",
    ")\n",
    "emoji_length_distribution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.plot(emoji_length_distribution.emoji_length, emoji_length_distribution.freq, \"o\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"#emojis in a tweet\")\n",
    "ax.set_ylabel(\"frequency of tweets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the most popular emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_matrix.sum(axis=0).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding frequent itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(emoji_matrix, min_support=0.005, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_frequent_itemsets(emoji_matrix, min_support=0.005, k=3):\n",
    "    frequent_itemsets = apriori(\n",
    "        emoji_matrix, min_support=min_support, use_colnames=True\n",
    "    )\n",
    "    return frequent_itemsets[frequent_itemsets[\"itemsets\"].apply(lambda x: len(x)) == k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_frequent_3itemsets = emoji_frequent_itemsets(emoji_matrix, min_support=0.005, k=3)\n",
    "# You can uncomment the following line to view the obtained frequent itemsets.\n",
    "emoji_frequent_3itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_frequent_4itemsets = emoji_frequent_itemsets(emoji_matrix, min_support=0.005, k=4)\n",
    "emoji_frequent_4itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People have developed various measurements of the interestingness of patterns. Most of them split the itemset into an antecedent item(set) and a consequent item(set), and then measure the correlation between the antecedent and the consequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interestingness_measurements = association_rules(\n",
    "    frequent_itemsets,\n",
    "    metric=\"lift\",\n",
    "    min_threshold=0,\n",
    ")\n",
    "\n",
    "interestingness_measurements.sort_values(\"support\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the full mutual information\n",
    "\n",
    "Full mutual information is defined as:\n",
    "\n",
    "$$I(X;Y)=\\sum_{x\\in\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}} P(X=x, Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(Y=y)}.$$\n",
    "\n",
    "Note that the logorithm requires that the joint probability $P(X=x, Y=y) > 0$, which does not hold for some $(x, y)$. However, since we know that when $P(X=x, Y=y) = 0$, it would not contribute to the sum, you may assume $P(X=x, Y=y)\\log_2\\frac{P(X=x,Y=y)}{P(X=x)P(P=y)} = 0$ in that case.\n",
    "\n",
    "$x$, $y$ are possible values of $X$ and $Y$; in the case of appearance or absence of an item, 1 or 0. Therefore, we need to consider all possible combinations of $x$ and $y$, that is, $(X=1, Y=1)$, $(X=1, Y=0)$, $(X=0, Y=1)$, $(X=0, Y=0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum(px: float, py: float, pxy: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the sum for a given combination of px, py, and pxy.\n",
    "\n",
    "    Parameters:\n",
    "    px (float): The probability of x.\n",
    "    py (float): The probability of y.\n",
    "    pxy (float): The joint probability of x and y.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated sum.\n",
    "    \"\"\"\n",
    "    return 0 if pxy == 0 else np.multiply(pxy, np.log2(pxy / (px * py)))\n",
    "\n",
    "\n",
    "def mi(antecedent_support: float, consequent_support: float, support: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mutual information.\n",
    "\n",
    "    Parameters:\n",
    "    antecedent_support (float): The support of the antecedent.\n",
    "    consequent_support (float): The support of the consequent.\n",
    "    support (float): The joint support of the antecedent and consequent.\n",
    "\n",
    "    Returns:\n",
    "    float: The mutual information.\n",
    "    \"\"\"\n",
    "    px1 = antecedent_support\n",
    "    px0 = 1 - antecedent_support\n",
    "    py1 = consequent_support\n",
    "    py0 = 1 - consequent_support\n",
    "\n",
    "    px1y1 = support\n",
    "    px1y0 = px1 - px1y1\n",
    "    px0y1 = py1 - px1y1\n",
    "    px0y0 = 1 - px1 - py1 + px1y1\n",
    "\n",
    "    px1y1_sum = calculate_sum(px1, py1, px1y1)\n",
    "    px1y0_sum = calculate_sum(px1, py0, px1y0)\n",
    "    px0y1_sum = calculate_sum(px0, py1, px0y1)\n",
    "    px0y0_sum = calculate_sum(px0, py0, px0y0)\n",
    "\n",
    "    return px1y1_sum + px1y0_sum + px0y1_sum + px0y0_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `mi` function, we can now compute the mutual information for each (antecedent -> consequent) pair and attach it to the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interestingness_measurements[\"mi\"] = interestingness_measurements.apply(\n",
    "    lambda pair: mi(\n",
    "        pair[\"antecedent support\"], pair[\"consequent support\"], pair[\"support\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "interestingness_measurements.sort_values(\"mi\", ascending=False).head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set_a, set_b):\n",
    "    union = set_a.union(set_b)\n",
    "    intersection = set_a.intersection(set_b)\n",
    "\n",
    "    return len(intersection) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.loc[0].emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
